# ETL Pipeline Project Using (AWS Lambda + S3)

## ğŸš€ Project Overview

This repository contains a **serverless data engineering ETL pipeline** designed to clean, standardize, and transform a raw **movies.csv** dataset stored in Amazon S3.  
The pipeline is implemented entirely using **AWS Lambda**, **Python**, **pandas**, and **S3**, following modern, cloud-native data engineering practices.

The goal of this project is to demonstrate how to build a **production-ready, scalable, event-driven ETL workflow** using AWS services â€” suitable for real deployments, portfolio projects, and enterprise data pipelines.

---

## ğŸ¯ Key Objectives

- Build a **fully automated ETL pipeline** using serverless architecture  
- Read raw CSV data from an S3 bucket (full-load processing)  
- Clean and transform the dataset using `pandas`  
- Enforce data quality rules and standardization  
- Store the processed dataset in a target S3 bucket  
- Produce detailed logs that illustrate every stage of the transformation  
- Demonstrate practical skills in **cloud data engineering**

---

## ğŸ— Architecture

The ETL pipeline follows a simple but powerful serverless design:

